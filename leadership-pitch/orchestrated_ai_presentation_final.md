---
title: "Orchestrated AI Teams: The Future of Research Excellence"
author: "Joe Hays, NRL Code 8234"
date: "November 2025"
---

## Executive Summary




**Critical Decision**: Embrace orchestrated AI or risk irrelevance

**The Progression**:
- üöó Traditional PhD = Corvette (baseline)
- üèéÔ∏è PhD + LLM Chat = Formula 1 (21-26% faster)
- ‚úàÔ∏è PhD + Coding Agents = Cessna (40-55% faster)
- üöÄ PhD + Manual Orchestration = Fighter Jet (100-150% faster)
- üõ∏ PhD + LangGraph = **Starship Enterprise** (200-400% faster)




**The Ask**:

1. **Primary**: Commit to organizational investment in orchestrated AI
2. **Secondary**: Consider MARS as the platform

**Evidence**: Peer-reviewed 2024 studies show **transformational** gains




---

## Table of Contents

### Part 1: The Existential Challenge
### Part 2: The AI Acceleration Ladder
### Part 3: Technology Primer
### Part 4: The Opportunity
### Part 5: MARS Prototype Solution
### Appendices


# Part 1: The Existential Challenge

---

## The Research Acceleration Crisis




**The Numbers**:
- **Daily output**: ~9,700 STEM papers/day
- **Human capacity**: 2-3 papers/day
- **Coverage**: <1%

**Core Advantage**:

> Researchers + orchestrated AI = **2-5√ó faster from idea to publication**




**Why Speed Matters**:
- First-mover advantage
- Compounding returns
- Talent retention
- Resource efficiency (2√ó speed = 50% cost per result)




---

## The Information Overload Gap

**Daily Papers**: 9,700 published
**Human Capacity**: 2-3 readable
**Coverage**: <1%

**Result**: Missing 99% of relevant breakthroughs

---

## What Happens Without Adaptation




**Historical Parallels** (2024):

**Software Development**:
- ‚úÖ AI-augmented: 40-55% ‚Üë
- ‚ùå Traditional: Talent loss

**Professional Services**:
- ‚úÖ AI-augmented: 30-40% ‚Üë
- ‚ùå Traditional: Losing bids








**Research Sector** (emerging):
- ‚úÖ AI-augmented: 2-3√ó publications
- ‚ùå Traditional: Falling citations
- ‚ùå Grants: "missed work" penalties

**Timeline**: **12-18 months** before gap becomes irreversible




---

## The Widening Gap




**WITH Orchestrated AI**:
- 90%+ literature coverage
- 3-5√ó faster breakthroughs
- Top talent attraction








**WITHOUT**:
- Perpetually "catching up"
- Declining grant success
- Talent drain




**Critical Window**: We are at **Month 6-8** of 18-month window

---

## The Competitor Landscape

**Who's Already Moving** (2024):

| Sector | Organizations | Status |
|--------|--------------|--------|
| **Government** | DARPA, DOE Labs, NIST | Deployed 2024 |
| **Academic** | MIT, Stanford, Berkeley | Scaling pilots |
| **Private** | DeepMind, Microsoft Research | Production |
| **Defense** | Lockheed, Boeing, Northrop | Deployed 2023-24 |

**What They're Building**: Literature monitoring, knowledge graphs, experiment design, orchestration layer ‚Üê **Key differentiator**


# Part 2: The AI Acceleration Ladder

---

## The Five Levels: Visual Overview

![AI Acceleration Ladder](diagrams/png/ai-acceleration-ladder.png){ width=90% }

---

## Level 0: Traditional PhD Teams (Corvette)




**Time Allocation**:
- High-Value Analysis: 30% (12 hrs)
- Literature Review: 20% (8 hrs)
- Writing/Docs: 30% (12 hrs)
- Experiment Setup: 20% (8 hrs)

**Problem**: Only **30%** on breakthrough work




**Baseline Metrics**:
- Literature coverage: **<5%**
- Publication velocity: **1√ó**
- Team effective size: **1√ó headcount**

**Constraints**:
- Fixed human reading speed
- 24-hour days
- Biological limits




---

## Level 1: PhD + LLM Chat (Formula 1)




**Tools**: ChatGPT, Claude, Gemini

**Evidence** (2024):
- Google: **21% faster**
- GitHub Copilot: **26% productivity ‚Üë**

**Improved**:
- Routine tasks: **+21-26%**
- High-value time: **~35-38%**
- Publication: **1.15-1.20√ó**








**Limitations**:
- ‚ùå No memory between sessions
- ‚ùå No tool integration
- ‚ùå Manual coordination
- ‚ùå Copy-paste overhead

**Use Case**: Simple Q&A, one-off tasks




---

## Level 2: PhD + AI Coding Agents (Cessna)




**Tools**: Claude Code CLI, GitHub Copilot, Cursor, Devin

**Key**: Agents **execute**, not just advise

**Evidence** (2024):
- Science: **40% faster**, **18% higher quality**
- GitHub: **55.8% speed ‚Üë**
- Capgemini: **30-40% time ‚Üì**








**Improved**:
- Coding speed: **1.75-2.00√ó**
- High-value time: **45-50%**
- Publication: **1.40-1.60√ó**
- Code quality: **+18%**

**Capabilities**:
- Autonomous execution
- Tool integration
- Error recovery




---

## Level 3: PhD + Manual Orchestration (Fighter Jet)




**Architecture**: Multiple specialized agents in parallel

**Example**:

Sequential (13 hrs):
- Lit Review ‚Üí Code ‚Üí Test ‚Üí Docs
- 4 hrs ‚Üí 6 hrs ‚Üí 2 hrs ‚Üí 1 hr

Parallel (8 hrs):
- Agent A: Lit (4 hrs)
- Agent B: Code (6 hrs)  } ‚Üí Merge (2 hrs)
- Agent C: Test (2 hrs)
- Agent D: Docs (1 hr)

**38% faster**




**Improved**:
- Parallel: **3-5 tasks** simultaneous
- High-value time: **60-65%**
- Publication: **2.00-2.50√ó**

**Limitations**:
- ‚ùå **3-4 hrs/day** coordination
- ‚ùå Human bottleneck (max 3-5)
- ‚ùå Manual integration
- ‚ùå Exhausting after 2-3 hours




---

## Level 4: PhD + LangGraph (Starship Enterprise)




**Key**: **Automated coordination**

**Evidence** (2024):
- McKinsey: **30-40% gains** beyond single-agent
- BCG: **45% margin ‚Üë**
- Total: **200-400% vs. baseline**








**Improved**:
- Overhead: **3-4 hrs/day ‚Üí 30 min**
- Parallel: **10-20+ tasks**
- High-value time: **75-80%**
- Publication: **3.00-5.00√ó**
- Coverage: **90%+**




**Difference**: Orchestrator handles coordination, human provides strategy only

---

## Orchestration Architecture

![Orchestration Flow](diagrams/png/orchestration-flow.png){ width=85% }

**Orchestrator's Job**:
1. Decompose complex task ‚Üí subtasks
2. Assign to specialized agents
3. Route information between agents
4. Synthesize outputs
5. Escalate strategic decisions

---

## Evidence Summary: 2024 Studies

| Level | Gain | Source Quality | Sample Size |
|-------|------|---------------|-------------|
| **Level 1** (Chat) | +21-26% | High (peer-reviewed) | 4,000+ |
| **Level 2** (Agents) | +40-55% | High (peer-reviewed) | 1,000+ |
| **Level 3** (Manual) | +100-150% | Medium (case studies) | <100 |
| **Level 4** (LangGraph) | +200-400% | Medium (industry) | <50 |

**Key Studies**: GitHub Copilot RCT, Science Magazine, McKinsey, BCG

**Takeaway**: Even **conservative** estimates show **transformational** gains


# Part 2.7: Concrete Use Cases

---

## What Orchestrated AI Can Do For You

**Purpose**: Ground the discussion in **practical, real-world capabilities**




**Today** (6 operational):
- Literature management (85-90% savings)
- Documentation automation (75-85% savings)
- Knowledge graph tracking
- Experiment logging (90% savings)
- Semantic code search
- Diagram generation (83-90% savings)








**Q1-Q2 2025** (7 planned):
- Literature surveillance (90%+ coverage)
- Gap analysis
- Research orchestration
- Plan authoring
- Code development (95% accuracy)
- Codebase analysis
- Documentation maturity




**Note**: This is just the beginning‚Äînew capabilities added in 3-7 weeks

---

## Operational: Literature Management

**Think of this as your personal librarian**






**What it does**:
- Find papers (topic, author, date)
- Organize in collections
- Cite properly (APA, IEEE, etc.)
- Summarize (AI reads 20-30 papers)








**Example**: Battery electrode proposal

- Traditional: 20-25 hrs
- With MARS: 2-3 hrs
- **Savings**: 85-90%

**You**: Describe what you need
**MARS**: Finds, reads, summarizes with citations






---

## Operational: Documentation Automation

**Think of this as a technical writer that never sleeps**






**What it does**:
- Generate docs from code
- Add citations to papers
- Create diagrams (architecture)
- Check links, gaps
- Format to standards








**Example**: New algorithm docs

- Traditional: 8-12 hrs
- With MARS: 1-2 hrs (review)
- **Savings**: 75-85%

**You**: Write the code
**MARS**: Documents, cites, diagrams






---

## Operational: Knowledge Graph

**Think of this as a research detective**






**What it does**:
- Trace paper ‚Üí decision
- Connect requirement ‚Üí result
- Discover cross-domain links
- Preserve institutional knowledge
- Answer "why this choice?"








**Example**: "Which paper inspired this?"

**MARS shows**:
- Paper A ‚Üí Requirement B
- Requirement B ‚Üí Design C
- Design C ‚Üí Experiment D
- Experiment D ‚Üí Result E

**Survives** personnel turnover






---

## Operational: Experiment Tracking

**Think of this as a lab notebook that writes itself**






**What it does**:
- **Log** parameters, settings
- **Record** results, metrics
- **Timestamp** with provenance
- **Store** searchable format
- **Enable** perfect reproducibility








**Example**: 50 experiments, 6 months later

**You**: "Reproduce experiment #23"
**MARS**: All parameters recorded
- Same settings
- Same data
- Same environment
- **Success**: First try

**Savings**: 90% time on tracking




---

## Operational: Semantic Search

**Think of this as search that understands meaning**






**What it does**:
- **Understand** intent, not keywords
- **Find** different terminology
- **Reduce** AI context 40%
- **Remember** across sessions
- **Ground** in codebase (fewer hallucinations)








**Example**: "Where do we handle authentication errors?"

**Keyword search**: Misses `handle_auth_failure`
**MARS semantic**: Finds it (same meaning)

**Benefits**:
- Faster results
- Lower costs
- Better accuracy




---

## Operational: Diagram Generation

**Think of this as an artist who draws what you describe**






**What it does**:
- **Generate** SysML/UML diagrams
- **Render** publication-quality
- **Update** when system changes
- **Maintain** consistent notation
- **Version** with code (git)








**Example**: "Sensor data flow diagram"

- Traditional: 3-5 hrs (PowerPoint)
- With MARS: 15 min
- **Savings**: 83-90%

**You**: Describe in English
**MARS**: Professional diagram




---

## Planned: Literature Surveillance

**Think of this as a 24/7 news alert service**






**What it does** (nightly):
- **Monitor** arXiv, PubMed, journals
- **Scan** 9,700+ papers/day
- **Filter** to 10-15 relevant
- **Summarize** with AI
- **Alert** each morning








**Example**: "Lithium batteries" interest

**Morning digest**:
- 47 battery papers published
- 8 most relevant highlighted
- AI summaries for each

**Coverage**: 90%+ vs. <5% manual
**You sleep**: MARS watches




---

## Planned: Gap Analysis

**Think of this as a research consultant**






**What it does**:
- **Analyze** hundreds of papers
- **Identify** untried approaches
- **Recommend** background reading
- **Suggest** citations
- **Connect** cross-domain ideas








**Example**: "What haven't we tried for battery capacity?"

**MARS analyzes** 500 papers:
- 3 unexplored approaches
- 12 papers to read
- Citation recommendations

**Traditional**: Weeks of manual review




---

## Planned: Research Orchestration

**Think of this as a project manager**






**What it does**:
- **Decompose** goal ‚Üí phases
- **Assign** tasks to agents
- **Coordinate** collaboration
- **Alert** at decision points
- **Track** and adapt








**Example**: "Develop new electrode material"

**MARS breaks down**:
1. Literature review
2. Simulation
3. Synthesis
4. Testing

**You**: Review milestones, decide strategy
**Agents**: Execute tasks




---

## Planned: Plan Authoring

**Think of this as a grant writer**






**What it does**:
- **Query** literature
- **Generate** plan (milestones, methods)
- **Include** expected outcomes
- **Format** to template
- **Draft** complete sections








**Example**: Research funding proposal

**You**: High-level goals
**MARS**: 10-page plan
- Background
- Methods
- Timeline
- Outcomes

**You refine**: Days vs. weeks




---

## Planned: Code Development

**Think of this as a programmer following blueprints**






**What it does**:
- **Generate** formal spec
- **Review** spec with you
- **Implement** from spec (95% accuracy)
- **Write** tests
- **Validate** correctness








**Example**: "Process sensor data, filter noise, detect anomalies"

**You write**: 2-page spec
**MARS**:
- Generates code
- Writes tests
- Validates

**You**: Review spec + final code




---

## Planned: Codebase Analysis

**Think of this as a tour guide for code**






**What it does**:
- **Analyze** structure
- **Generate** diagrams
- **Explain** in plain English
- **Answer** implementation questions
- **Reduce** learning time (days ‚Üí hrs)








**Example**: 50,000-line inherited codebase

**You**: "How does authentication work?"
**MARS**:
- Sequence diagram
- Component explanations
- Flow walkthrough

**Learning**: 30 min vs. 2 days




---

## Planned: Documentation Maturity

**Think of this as a copy editor**






**What it does**:
- **Scan** all documents
- **Detect** gaps, broken links
- **Generate** drafts
- **Update** citations (Zotero)
- **Produce** publication-ready








**Example**: 50 project documents, incomplete

**MARS**:
- 15 missing sections identified
- 12 drafts generated
- 47 broken links fixed
- All citations updated

**You**: Review vs. weeks tracking




---

## Future: Robotics Integration

**Think of this as a robotics engineer you talk to**






**What it will do**:
- **Collect** ROS2 sensor data
- **Preprocess** datasets
- **Submit** GPU training jobs
- **Validate** in simulation
- **Deploy** to robot hardware








**Example**: "Train navigation policy from yesterday's data"

**One sentence** ‚Üí
- Find ROS2 bags
- Preprocess
- Train (4 GPUs, 8 hrs)
- Test (Isaac-Lab)
- Deploy

**Reduction**: 80% manual Python coding




---

## Future: HPC Workflows

**Think of this as a supercomputer scheduler**






**What it will do**:
- **Design** pipelines
- **Schedule** jobs (SLURM)
- **Monitor** progress
- **Optimize** resources
- **Alert** when ready








**Example**: 1,000 simulations (parameter sweep)

**You describe** what you want
**MARS**:
- Nextflow/Snakemake setup
- Job scheduling
- Progress monitoring
- Results notification

**No scripts**, no babysitting




---

## What This Means For You: Summary




**Today** (6 operational):
- 85-90% time savings (literature)
- 75-85% reduction (docs)
- Perfect reproducibility
- Institutional memory

**Q1-Q2 2025** (7 v1.0):
- 90%+ literature coverage
- Multi-month orchestration
- 95% code accuracy
- Publication-grade docs








**Future** (2+ expansion):
- Robotics workflows (80% reduction)
- HPC pipelines (conversational)
- **Your domain needs** (3-7 weeks)

**Key Insight**:
> Every research group will discover 5-10 new use cases we haven't imagined yet

**Platform grows with your needs**





# Part 3: Technology Primer

---

## What is an LLM?




**Simple**: Pattern-matching engine trained on billions of pages

**Think**: Research assistant who read every paper ever written

**How**:
1. Trained on billions of pages
2. Learns patterns
3. Predicts next words
4. Result: Human-like text








**Good At** ‚úÖ:
- Summarization, translation
- Q&A, code generation
- Pattern recognition

**Not Good At** ‚ùå:
- Original discovery
- Precise calculation
- Long-term memory
- Tool use (basic LLMs)




---

## The Memory Ladder

**The Progression**:
- üìù **Level 0**: Post-It Notes (ChatGPT - no memory)
- üìì **Level 1**: Personal Notebook (session only)
- üìñ **Level 2**: Reference Manual (CLAUDE.md - static context)
- üóÇÔ∏è **Level 3**: Card Catalog (RAG/semantic search)
- üìö **Level 4**: Cross-References (knowledge graphs)
- üè´ **Level 5**: University Library (OpenMemory - learns)
- üèõÔ∏è **Level 6**: Library of Congress (institutional memory)

![Memory Ladder](diagrams/png/memory-ladder.png){ width=85% }

---

## Level 0: Post-It Notes (ChatGPT)

üìù **The Analogy**: Post-It notes you throw away after each conversation

**Characteristics**:
- **No memory** between conversations
- Starts from scratch every time
- No context preservation
- Must re-explain everything

**Example**: ChatGPT basic chat
- You: "I'm working on battery research"
- *Close browser*
- You: "What were we discussing?"
- ChatGPT: "I don't recall previous conversations"

**Why This Level**: Stateless interaction, no persistence

---

## Level 1: Personal Notebook (Session Memory)

üìì **The Analogy**: Notebook that disappears when you close it

**Characteristics**:
- **Session memory only**
- Remembers within single conversation
- Forgets when session ends
- No cross-session learning

**Example**: ChatGPT with conversation history
- You: "I'm researching lithium batteries"
- *Later in same session*
- You: "What are the challenges we discussed?"
- ChatGPT: "The lithium battery challenges you mentioned were..."
- *Close browser ‚Üí memory lost*

**Improved vs Level 0**: Within-session context, but still ephemeral

---

## Level 2: Reference Manual (CLAUDE.md)

üìñ **The Analogy**: Operations manual everyone reads before starting

**Characteristics**:
- **Static context** loaded at session start
- Consistent behavior across sessions
- Project-specific knowledge
- No learning or adaptation

**Example**: MARS CLAUDE.md
- Every Claude Code session reads CLAUDE.md
- Knows MARS structure, commands, policies
- Behavior consistent across all sessions
- But: doesn't remember YOUR previous work

**MARS Status**: ‚úÖ **Level 2 Complete** (CLAUDE.md + policy bundles)

---

## Level 3: Library Card Catalog (RAG/Semantic Search)

üóÇÔ∏è **The Analogy**: Library where you can find any book by topic

**Characteristics**:
- **RAG/Semantic Search** (~40% token reduction)
- Automatic context retrieval
- Searches by meaning, not keywords
- Finds relevant information on-demand

**Example**: MARS claude-context MCP
- You: "Where do we handle authentication?"
- MARS searches codebase semantically
- Retrieves relevant code sections automatically
- No need to remember exact file paths

**MARS Status**: ‚è≥ **80% Complete** (infrastructure ready, blocked by upstream bug)

---

## Level 4: Library with Cross-References (Knowledge Graphs)

üìö **The Analogy**: Library where books reference each other with annotations

**Characteristics**:
- **Knowledge Graphs** (Neo4j)
- Relationship understanding
- Traces paper ‚Üí requirement ‚Üí experiment ‚Üí result
- Cross-domain connection discovery

**Example**: MARS Knowledge Graph
- You: "Which paper inspired this design decision?"
- MARS: "Paper A (2023) ‚Üí Requirement B ‚Üí Design C ‚Üí Experiment D"
- Preserves institutional knowledge
- Survives researcher turnover

**MARS Status**: ‚úÖ **Infrastructure Complete** (agent integration pending)

---

## Level 5: University Library (OpenMemory)

üè´ **The Analogy**: University library that learns your research interests

**Characteristics**:
- **OpenMemory architecture** (5 memory sectors)
- **Conversation**: What you just said
- **Session**: This work session's context
- **Episodic**: Past project milestones
- **Entity**: People, papers, concepts
- **Semantic**: General knowledge learned

**Example**: Multi-month research project
- Remembers your research goals
- Tracks ongoing experiments
- Recalls past decisions and why
- Learns your preferences over time

**MARS Status**: ‚è∏Ô∏è **Planned v1.5+** (4-6 weeks implementation)

---

## Level 6: Library of Congress (Full Institutional Memory)

üèõÔ∏è **The Analogy**: National library preserving all institutional knowledge

**Characteristics**:
- **Full institutional memory**
- Shared across all agents
- Survives researcher turnover
- Organization-wide knowledge base
- Multi-year persistence

**Example**: Research organization memory
- New researcher joins team
- MARS: "Your predecessor worked on X, here's why approach Y failed"
- Knowledge doesn't walk out the door
- 10-year institutional memory preserved

**MARS Status**: ‚è∏Ô∏è **Proposed v2.0+** (Major architectural effort)

---

## Memory Ladder: Why It Matters

**The Progression**:
- **Level 0-1**: Forget everything (start over each time)
- **Level 2**: Read the manual (consistent but static)
- **Level 3**: Search efficiently (find what you need)
- **Level 4**: Understand relationships (connect the dots)
- **Level 5**: Learn over time (adapt to your work)
- **Level 6**: Institutional memory (survive turnover)

**MARS Today**: Levels 2-4 operational (2-3-4 triangle)
**MARS v1.0**: Level 3 fully operational (40% token reduction)
**MARS v1.5+**: Level 5 (adaptive learning)
**MARS v2.0+**: Level 6 (institutional memory)

---

## What is an AI Agent?




**Definition**: LLM + Tool Use + Multi-Step Planning

**Lab Analogy**:
- **LLM (Chat)** = Consultant (advises, leaves)
- **AI Agent** = Postdoc (executes, works autonomously)




**What Agents Do**:
- Read/write files
- Execute code, run tests
- Query databases
- Multi-step planning
- Autonomous work (hours)

**Why Level 2**: Autonomous execution, tool integration, error recovery, but one task at a time




---

## What is MCP?




**Model Context Protocol** = USB for AI agents

**Before MCP**: Custom integration = **40-80 hours**
**After MCP**: MCP server = **<1 hour**

**Value**:
- Ecosystem, not custom
- No vendor lock-in
- Open standard








**MARS MCP Servers**:
- ‚úÖ Zotero (lit mgmt) - Operational
- ‚úÖ GitLab (79+ tools) - Operational
- ‚è∏Ô∏è **50+ planned**: ROS2, SLURM, Overleaf, LabView, MATLAB, SolidWorks, eLabFTW, PubMed, IEEE, arXiv, Benchling, etc.




---

## What is AI Orchestration?




**Definition**: Automated coordination of specialized AI agents

**Lab Analogy**:
- **Manual**: You coordinate (**3-4 hrs/day**)
- **Automated**: AI coordinator (**30 min/day**)








**LangGraph Process**:
1. Decompose ‚Üí subtasks
2. Assign to agents
3. Route information
4. Synthesize outputs
5. Escalate decisions

**Result**: Human = strategy, orchestrator = tactics




---

## Why Teams Beat Single Agents




**Specialization**:
- Single = Generalist (context switching, errors)
- Team = Specialists (focused, quality)

**Agent Profiles**:
- **test-czar**: Skeptical (finds edge cases)
- **planner**: Pragmatic (feasibility)
- **research-orchestrator**: Optimistic (breakthroughs)
- **doc-enforcer**: Pedantic (publication quality)




**Evidence**: McKinsey **30-40% gains** beyond single-agent

**Mechanism**:
1. Specialization: +20-30%
2. Parallelization: +25-35%
3. Coordination: +25-40%
4. **Compounding**: Multiplicative





# Part 4: The Opportunity

---

## Become a "Starship Enterprise" Organization




**Current State** (Corvette ‚Üí F1):
- ‚ùå Occasional ChatGPT use
- ‚ùå Some early adopter agents
- ‚ùå No strategy
- ‚ùå No infrastructure








**Where We Could Be** (12 mo):
- ‚úÖ Every group has orchestrated AI
- ‚úÖ Literature automated (90%+)
- ‚úÖ Experiment design AI-augmented
- ‚úÖ Publication **3-5√ó baseline**
- ‚úÖ Competitive moat




---

## Daily Workflow Vision

| Time | Activity | Human Role | AI Role |
|------|----------|-----------|---------|
| **Morning** (15 min) | Literature | Review + approve | 1,500+ papers ‚Üí 10-15 relevant |
| **Mid-day** (4-6 hrs) | **High-value** | Design, interpret, write | Code, lit, data, docs |
| **Afternoon** (2-3 hrs) | Collaboration | Meetings, synthesis | Agent output review |
| **Evening** (auto) | Maintenance | None (sleeping) | Lit scrubbing, sims, backups |

**Time Shift**: **30% ‚Üí 75%** on breakthrough work

---

## Competitive Advantage




**WITH Orchestrated AI**:
- More literature (**90% vs. 5%**)
- Faster publication (**3-5√ó**)
- Higher quality proposals








**WITHOUT**:
- Declining grant success
- Talent drain
- Slower breakthroughs




**Context**: Compete against **5-10√ó our headcount**
**Solution**: **Force multiplication** via orchestrated AI

---

## Accelerating Breakthroughs




**1. Cross-Domain Synthesis**
- Monitor multiple domains
- Identify unexpected connections
- Example: ML method ‚Üí materials sim

**2. Non-Obvious Patterns**
- Analyze 1,500+ papers/day
- Detect statistical trends
- Example: "Method B citations +300%"








**3. Rapid Prototyping**
- Test 10√ó more hypotheses/year
- Proof-of-concept in days
- Fail fast, pivot quickly

**4. Avoiding Dead-Ends**
- Comprehensive prior work
- Identify showstoppers BEFORE 6-month investment
- Example: "Parameter X causes instability"





# Part 5: MARS Prototype Solution

---

## How I've Been Preparing




**Who I Am**: Intelligent autonomous systems researcher

**"Sharpening the Saw"**:
- Literature Review: 40%
- Documentation: 30%
- **Actual Research: 20%**
- Writing: 10%

**This was backwards.**








**Decision**: Build research-first platform

**Timeline**:
- August 2025: Started (self-funded)
- Sep-Nov 2025: Intensive dev
- **Current**: Foundation complete

**Investment**: ~800-1,000 hours over 3-4 months




---

## What is MARS?

**Modular Agentic Research System** = OS for AI-accelerated R&D




**Components**:
1. **Foundation**: Docker, Neo4j, Milvus, MLflow
2. **AI Integration**: LiteLLM, Ollama
3. **Research Tools**: Zotero, GitLab, PlantUML/SysML








4. **Agents**: DocCzar, TestCzar, KG, orchestrator
5. **Orchestration**: LangGraph foundation

**Why Self-Hosted**:
- Data privacy (never leaves network)
- Air-gap capable
- No vendor lock-in
- Cost control, customization




---

## The 8-Pillar Foundation

| Pillar | Description | Why Critical |
|--------|-------------|-------------|
| **P1: Modularity** | "Hotel rooms" | Add in 3-7 weeks (not 6-12 mo) |
| **P2: Security** | Sysbox, DoD | Classified-capable, air-gap |
| **P3: Memory** ‚≠ê | KG, RAG | **MOST IMPORTANT** - 40% tokens |
| **P4: Observability** | Provenance | Full traceability |
| **P5: Reproducibility** | Containerized | Experiment replay |
| **P6: Human-AI** | Approval gates | Safety, trust |
| **P7: Air-Gap** | 100% offline | Classified networks |
| **P8: Open Standards** | MCP, Docker | No lock-in, ecosystem |

**P3 is Key**: Without memory = tools. With memory = research accelerators.

---

## MARS Architecture

![MARS Runtime Architecture](diagrams/png/mars-rt-architecture.png){ width=85% }

---

## Two Faces of MARS

**MARS has two distinct instantiations with different purposes:**

::::: {.columns}
:::: {.column width="48%"}

**mars-dev (The Workshop)**
- üîß **Purpose**: Tools for building MARS itself
- üèóÔ∏è **Users**: MARS developers (me)
- üì¶ **Contains**: Development infrastructure
  - Parallel orchestration (E8)
  - Session management
  - Pre-commit hooks
  - Testing infrastructure
  - Development CLIs
- üìç **Location**: `mars-dev/` directory
- üéØ **Goal**: Make MARS better

::::
:::: {.column width="48%"}

**research-project (The Product)**
- üî¨ **Purpose**: Using MARS for research
- üë®‚Äçüî¨ **Users**: Research teams (you)
- üì¶ **Contains**: MARS as submodule + your work
  - MARS framework (read-only)
  - Research algorithms
  - Experiment scripts
  - Data and results
- üìç **Location**: Your repository
- üéØ **Goal**: Accelerate your research

::::
:::::

**Key Distinction**: mars-dev builds MARS, research-project uses MARS

---

## Research Project Structure

**How you'll use MARS:**

```
your-research-project/           # Your repository
‚îú‚îÄ‚îÄ .mars/                       # MARS configuration overrides
‚îÇ   ‚îú‚îÄ‚îÄ config/                  # Project-specific settings
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.override.yml  # Customize services
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ framework/               # MARS submodule (read-only)
‚îÇ   ‚îî‚îÄ‚îÄ algorithms/              # Your research code
‚îú‚îÄ‚îÄ data/                        # Your research data
‚îú‚îÄ‚îÄ results/                     # Your experiment results
‚îú‚îÄ‚îÄ notebooks/                   # Your Jupyter notebooks
‚îú‚îÄ‚îÄ scripts/                     # Your analysis scripts
‚îî‚îÄ‚îÄ README.md                    # Your documentation
```

**Critical Principle**: MARS framework is read-only. All customization goes in `.mars/` directory.

---

## Configuration: MARS vs. Your Project

**Two separate configuration mechanisms:**

::::: {.columns}
:::: {.column width="48%"}

**mars-dev Configuration**
- üîß **For**: Building MARS
- üìÇ **Location**: `mars-dev/` internal files
- ‚öôÔ∏è **Controls**: Development tools
  - E8 orchestration
  - Pre-commit hooks
  - Sprint templates
  - Test infrastructure
- üë§ **User**: MARS developers
- üö´ **Not in research-project**: Development infrastructure stays separate

::::
:::: {.column width="48%"}

**research-project Configuration**
- üî¨ **For**: Using MARS
- üìÇ **Location**: `.mars/` directory
- ‚öôÔ∏è **Controls**: MARS services
  - LLM models (which AI to use)
  - Service ports
  - Resource limits
  - Domain-specific agents
- üë§ **User**: Research teams
- ‚úÖ **Overrides MARS defaults**: Your project, your rules

::::
:::::

**Example**: Change Neo4j port for your project ‚Üí edit `.mars/docker-compose.override.yml`

---

## MARS External Dependencies

**MARS is built on 5 external repositories (git submodules):**

| Dependency | Type | Purpose | Status |
|------------|------|---------|--------|
| **LiteLLM** | Python/Docker | AI gateway (AskSage/CAPRA access) | ‚úÖ Production |
| **Zotero MCP** | Python MCP | Literature management tools | ‚úÖ Production (95% complete) |
| **GitLab MCP** | Node.js MCP | 40+ GitLab integration tools | ‚úÖ Production (50% complete) |
| **Zotero Server** | Docker Compose | Self-hosted reference server | ‚úÖ Evaluation |
| **Session Exporter** | Python CLI | CCC session archival | ‚úÖ Integrated |

**What "git submodule" means**: Version-controlled dependencies - reproducible builds, no surprises

**For researchers**: You don't manage these - they're pre-integrated

---

## MARS vs. mars-dev: The CLIs

**Two command-line interfaces with different audiences:**

::::: {.columns}
:::: {.column width="48%"}

**mars-dev CLI**
- üîß **For**: Building MARS
- üë§ **User**: MARS developers (me)
- üì¶ **Commands**: Infrastructure operations
  - `mars-dev up` - Start development container
  - `mars-dev down` - Stop development
  - `mars-dev build` - Build MARS images
  - `mars-dev doctor` - Health checks
  - `mars-dev validate` - Compliance checks
  - `mars-dev audit` - Module validation
- üèóÔ∏è **Use Case**: Develop new MARS features
- üìç **Location**: `mars-dev/scripts/mars-dev`

::::
:::: {.column width="48%"}

**mars CLI**
- üî¨ **For**: Running MARS
- üë§ **User**: Research teams (you)
- üì¶ **Commands**: Research operations
  - `mars up` - Start MARS services
  - `mars down` - Stop MARS services
  - `mars logs` - View service logs
  - `mars agents list` - List available agents
  - `mars services info` - Service details
  - `mars rag reindex` - Update knowledge graph
- üî¨ **Use Case**: Conduct research
- üìç **Location**: `core/scripts/mars`

::::
:::::

**You'll use**: `mars` CLI (research-project). You'll never need `mars-dev` CLI.

---

## How Research Projects Get MARS

**Simple 3-step process:**

**Step 1: Add MARS as submodule**
```bash
cd your-research-project
git submodule add https://github.com/nasa/mars-v2 src/framework
git submodule update --init --recursive
```

**Step 2: Create `.mars/` configuration**
```bash
mkdir -p .mars/config
# Customize MARS for your project (optional)
```

**Step 3: Start MARS**
```bash
cd src/framework
source mars-env.config  # Load MARS environment
mars up -d              # Start MARS services
```

**That's it!** MARS is running. Now write your research code.

---

## Documentation: Two Audiences

**Current State**: Documentation is messy (`docs/wiki/` will be reorganized)

**Future State**: Modular documentation architecture aligned with users

::::: {.columns}
:::: {.column width="48%"}

**For Developers (Building MARS)**
- üîß **Audience**: MARS developers (me)
- üìÇ **Location**: Within module directories
- üì¶ **Structure**:
  - `core/docs/` - Core framework
  - `mars-dev/docs/` - Dev tools
  - `modules/*/docs/` - Module-specific
- üìù **Content**:
  - ADRs (architecture decisions)
  - Implementation details
  - Development workflows
  - Testing strategies

::::
:::: {.column width="48%"}

**For Researchers (Using MARS)**
- üî¨ **Audience**: Research teams (you)
- üìÇ **Location**: Generated GitLab wiki
- üì¶ **Structure**:
  - Getting Started guides
  - Use case examples
  - Configuration reference
  - Troubleshooting
- üìù **Content**:
  - How to use MARS services
  - Research workflows
  - Agent capabilities
  - Integration examples

::::
:::::

**Key Principle**: Documentation lives where it's maintained, aggregated where it's consumed

---

## Modular Documentation Architecture

**Current mess** (everything in `docs/wiki/`):
```
docs/wiki/
‚îú‚îÄ‚îÄ adr/           # ALL 36 ADRs mixed together
‚îú‚îÄ‚îÄ integrations/  # Mixed module concerns
‚îú‚îÄ‚îÄ tools/         # Mixed concerns
‚îî‚îÄ‚îÄ ...            # 150+ files, unclear ownership
```

**Future organization** (modular, clear ownership):
```
core/docs/                    # Core framework docs
mars-dev/docs/                # Development infrastructure
modules/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ shared/docs/          # All agents
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator/docs/    # Specific agent
‚îî‚îÄ‚îÄ services/
    ‚îú‚îÄ‚îÄ shared/docs/          # All services
    ‚îî‚îÄ‚îÄ ollama/docs/          # Specific service
```

**Benefits**: Clear ownership, module independence, easy discovery

---

## Documentation Types & Organization

**Every module (`core/`, `mars-dev/`, `modules/*/`) has same structure:**

| Directory | Purpose | Example |
|-----------|---------|---------|
| **adr/** | Architecture decisions | Design choices, trade-offs |
| **educational/** | Learn the technology | Concepts, tutorials |
| **setup/** | Configuration & usage | Install, configure, run |
| **architecture/** | Implementation details | Internal design, patterns |
| **mcp/** | MCP tool reference | API documentation (services) |
| **testing/** | Test documentation | Test strategy, coverage |
| **troubleshooting/** | Fix issues | Common problems, solutions |

**ADR Numbering**: Per-directory (core/ADR-0001, mars-dev/ADR-0001, ollama/ADR-0001)

**Cross-references**: Use path for clarity ("`mars-dev/ADR-0004`" = Modular Documentation)

---

## Generated Wiki (Aggregation)

**Source of truth**: Documentation lives in module directories (git-tracked)

**GitLab wiki**: Aggregated view (generated automatically)

```
docs/wiki/                    # Generated, NOT hand-edited
‚îú‚îÄ‚îÄ adr/
‚îÇ   ‚îú‚îÄ‚îÄ core/                 # From core/docs/adr/
‚îÇ   ‚îú‚îÄ‚îÄ mars-dev/             # From mars-dev/docs/adr/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shared/           # From modules/agents/shared/docs/adr/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator/     # From modules/agents/orchestrator/docs/adr/
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îî‚îÄ‚îÄ ollama/           # From modules/services/ollama/docs/adr/
‚îú‚îÄ‚îÄ educational/              # Same structure
‚îú‚îÄ‚îÄ setup/                    # Same structure
‚îî‚îÄ‚îÄ cross-cutting/            # Strategic docs (VISION, ROADMAP)
    ‚îú‚îÄ‚îÄ VISION.md
    ‚îî‚îÄ‚îÄ ROADMAP.md
```

**Workflow**: Edit module docs ‚Üí Script aggregates ‚Üí GitLab wiki updates

**Benefit**: Single source of truth, multiple views (module-local, centralized wiki)

---

## Self-Hosted GitLab (Security)

**Why self-hosted GitLab inside MARS?**

**Security requirement**: Classified research code cannot leave the network

**Architecture**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MARS Runtime Container                  ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ GitLab CE (Self-Hosted)        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ - All repos stored locally     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ - No external network access   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ - Complete version control     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ - Issues, MRs, wikis           ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  Access: localhost:9080 (inside MARS)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits**: Air-gap capable, no data leakage, full git functionality

---

## GitLab Sync (Collaboration)

**Problem**: Self-hosted GitLab is isolated - how to collaborate with team?

**Solution**: Automated bidirectional sync with organizational GitLab

**Architecture**:
```
MARS GitLab (Isolated)  <---Sync--->  Org GitLab (External)
  localhost:9080                      gitlab.org.example.com
  - Classified research               - Unclassified/reviewed code
  - Daily development                 - Team collaboration
  - Full history                      - External review
```

**Sync Mechanism**:
- **260 automated tests**, 90%+ coverage
- **Bidirectional**: Pull team updates, push your work
- **Safe**: Agents stopped during sync, network temporarily enabled
- **Audited**: Complete operation logging

**User workflow**: `mars sync:pull` (morning), `mars sync:push` (evening)

---

## GitLab Sync Details

**Three key operations:**

**1. Pull updates** (get team changes):
```bash
mars sync:pull mars-v2
# 1. Stop agents (30s)
# 2. Enable network temporarily
# 3. Pull from org GitLab
# 4. Disable network (restore isolation)
# 5. Restart agents
# Duration: 30-90 seconds
```

**2. Push work** (share your commits):
```bash
mars sync:push mars-v2
# Same 5-step process, but pushes instead
```

**3. Check status** (see pending changes):
```bash
mars sync:status
# Shows: commits to pull, commits to push
```

**Safety**: Full audit logging, backup/rollback, metadata sync (issues/MRs/wikis)

---

## The Modularity Ladder

**The Progression**:
- üè† **Level 0**: Custom Home (6-12 months per capability - monolithic)
- üèóÔ∏è **Level 1**: Prefab Sections (3-6 months - brittle integration)
- üè¢ **Level 2**: Apartment Building (1-3 months - microservices)
- üè® **Level 3**: Modular Hotel (3-7 weeks per new domain - plug-and-play)

![Modularity Ladder](diagrams/png/modularity-ladder.png){ width=85% }

---

## Level 0: Custom Home (Monolithic)

üè† **The Analogy**: Building a custom house from scratch every time

**Characteristics**:
- **6-12 months per capability**
- Everything built from ground up
- No reusable components
- Tightly coupled code
- Hard to modify or extend

**Example**: Traditional AI research system
- Build authentication system
- Build storage system
- Build compute infrastructure
- Build AI integration
- Build user interface
- Every component custom-built and interconnected

**MARS Comparison**: Traditional approach takes 6-12 months with 3-5 FTE. MARS Level 3 delivers same capability in 3-7 weeks with 1-2 FTE.

**Problem**: Every new research domain requires rebuilding everything.

---

## Level 1: Prefab Sections (Brittle Integration)

üèóÔ∏è **The Analogy**: Using prefabricated walls but custom-fitting each connection

**Characteristics**:
- **3-6 months per capability**
- Some reusable components
- Manual integration required
- Brittle connections between parts
- Version conflicts common

**Example**: Docker containers with custom glue code
- Container for database
- Container for AI service
- Container for web server
- Custom scripts to connect them
- Configuration files manually synced

**Integration Issues**:
- Service A expects JSON, Service B sends XML
- Container 1 uses Python 3.8, Container 2 needs 3.11
- Network configs conflict between services
- Updates to one service break others

**MARS Comparison**: Level 3 uses standardized MCP protocol - no brittle glue code needed.

---

## Level 2: Apartment Building (Microservices)

üè¢ **The Analogy**: Shared foundation with independent units

**Characteristics**:
- **1-3 months per capability**
- Microservices architecture
- Shared infrastructure
- APIs for communication
- Better isolation than Level 1

**Example**: Docker Compose with multiple services
- Shared Docker network
- Shared storage volumes
- API-based communication
- Independent deployment of services

**MARS Partial Example**:
- Neo4j knowledge graph service
- MLflow experiment tracking service
- MinIO object storage service
- Services communicate via REST APIs

**Remaining Challenges**:
- Still need custom integration code
- API versioning complexities
- Service discovery overhead
- No standardized agent protocol

**MARS Comparison**: Level 3 adds MCP standardization - agents plug in instantly.

---

## Level 3: Modular Hotel (Plug-and-Play)

üè® **The Analogy**: Hotel with foundation built once, rooms added as needed

**Characteristics**:
- **3-7 weeks per new domain**
- Foundation built once and shared
- Plug-and-play modules
- Standardized interfaces (MCP protocol)
- 90% foundation reuse

**MARS "Hotel Rooms" Architecture**:

**Foundation (Built Once)**:
- Docker orchestration
- Neo4j knowledge graph
- MinIO object storage
- LiteLLM AI gateway
- Squid security proxy
- MLflow experiment tracking

**Modular "Rooms" (Add As Needed)**:
- Agents (lit-monitor, orchestrator, testczar)
- Services (Zotero, GitLab)
- Domain workflows (materials, chemistry, biology)

**Standardized Interfaces**:
- **MCP protocol**: Model Context Protocol for agent communication
- Agents register available tools/capabilities
- Other agents discover and invoke them
- No custom integration code needed

**Example - Adding Materials Domain**:
- Week 1: Use existing foundation (0 hrs)
- Weeks 2-4: Build materials agents (80-120 hrs)
- Weeks 5-7: Add domain tools (40-80 hrs)
- **Total: 3-7 weeks, 1-2 FTE**

**vs. Traditional**: 6-12 months, 3-5 FTE

**MARS Status**: ‚úÖ **Production-ready modular architecture**

---

## Modularity Ladder: Why It Matters

**Speed to New Capabilities**:
- Level 0: 6-12 months ‚Üí Level 3: 3-7 weeks (75% time savings)
- Level 0: 3-5 FTE ‚Üí Level 3: 1-2 FTE (50% cost savings)

**Reduced Risk**:
- Shared foundation = well-tested, proven infrastructure
- New modules inherit security, observability, reliability
- Failures isolated to individual modules

**Scaling Research**:
- Add new research domains rapidly
- Researchers focus on science, not infrastructure
- Cross-domain insights via shared knowledge graph

**Technology Evolution**:
- Swap AI models without rebuilding (LiteLLM abstraction)
- Upgrade infrastructure without touching research code
- MCP protocol enables future agent ecosystems

**Real-World Impact**:
- Materials group operational in 5-7 weeks (not 6-12 months)
- Chemistry group reuses 90% of infrastructure
- Biology group reuses same foundation
- Each new domain accelerates the next

---

## Modularity Example: Materials Group

**Timeline**: 5-7 weeks (vs. 6-12 months from scratch)

| Week | Activity | Effort | Notes |
|------|----------|--------|-------|
| **Week 1** | Use existing | 0 hrs | Zotero, GitLab, KG (immediate) |
| **Weeks 2-4** | Materials agents | 80-120 hrs | lit-monitor, KG schema, exp-design |
| **Weeks 5-6** | Custom tools | 40-80 hrs | LAMMPS, VASP integration |
| **Total** | **5-7 weeks** | **120-200 hrs** | **90% foundation reuse** |

**Cost Comparison**: Monolithic = 6-12 mo, 3-5 FTE | MARS = 5-7 wk, 1-2 FTE
**Savings**: 75% time, 50% FTE

---

## The Security Ladder

**The Progression**:
- üö™ **Level 0**: Open Door (public research only)
- üîë **Level 1**: Lock & Key (limited classified use)
- üèòÔ∏è **Level 2**: Gated Community (some classified with waivers)
- üèõÔ∏è **Level 3**: Military Base (DoD classified, air-gap capable)

![Security Ladder](diagrams/png/security-ladder.png){ width=85% }

---

## Level 0: Open Door (Public Research)

üö™ **The Analogy**: Building with no security - anyone can enter

**Characteristics**:
- **No security controls**
- Cloud-hosted AI (OpenAI, Anthropic)
- Data leaves your network
- Public internet access required
- No classification support

**Example**: ChatGPT Plus, Claude.ai
- Convenient but data goes to cloud
- Cannot handle CUI/classified
- Vendor controls your data
- Internet outage = no access

**Appropriate For**: Public research, open-source projects, unclassified work only

---

## Level 1: Lock & Key (Limited Classified)

üîë **The Analogy**: Building with basic locks, but windows still open

**Characteristics**:
- **Basic security** (authentication, HTTPS)
- Some data stays local
- Limited classified use (waivers required)
- Still dependent on external services
- Partial air-gap support

**Example**: GitHub Copilot Enterprise
- Data stays in your tenant
- Authentication required
- But: Still calls external APIs
- But: Limited control over model

**Appropriate For**: CUI (Controlled Unclassified Information), some sensitive research

---

## Level 2: Gated Community (Some Classified)

üèòÔ∏è **The Analogy**: Gated neighborhood with security checkpoints

**Characteristics**:
- **Enhanced security** (network isolation)
- Self-hosted infrastructure
- Some classified with waivers
- Reduced external dependencies
- Monitored access

**Example**: Private cloud deployments
- Self-hosted LLM infrastructure
- Network segmentation
- Access logging
- But: May still need external model APIs
- But: Not fully air-gap capable

**Appropriate For**: Sensitive research, some classified work with security waivers

---

## Level 3: Military Base (DoD Classified)

üèõÔ∏è **The Analogy**: Military installation with multiple security layers

**Characteristics**:
- **Production-ready for DoD classified**
- 100% self-hosted (no external calls)
- Air-gap capable
- Deny-by-default networking
- Full audit logging

**MARS Security Features**:

**Network Security**:
- **Squid proxy**: Deny-by-default (explicit allowlist)
- Air-gap capable (no internet required)
- All external calls blocked unless approved

**Container Security**:
- **Sysbox**: Rootless containers (UID/GID isolation)
- No privileged containers
- Kernel namespace isolation

**Authentication**:
- **Bearer token auth**: DoD PKI/CAC support
- Certificate-based authentication
- Role-based access control (RBAC)

**Data Security**:
- **100% self-hosted**: Local LLMs via Ollama
- No data leaves network
- Encrypted at rest and in transit

**Audit & Compliance**:
- **Append-only logging**: Full provenance tracking
- Tamper-proof audit trail
- Classification label propagation

**MARS Status**: ‚úÖ **Production-ready for classified networks**

---

## Security Ladder: Why It Matters

**The Progression**:
- **Level 0**: Convenient but insecure (cloud AI services)
- **Level 1**: Basic protection (some local control)
- **Level 2**: Enhanced security (mostly self-hosted)
- **Level 3**: DoD classified capable (full air-gap, zero trust)

**MARS Today**: Level 3 operational
- Air-gap deployments ready
- DoD classified network capable
- 100% self-hosted (no external dependencies)
- Deny-by-default security posture

**Why This Matters**:
- **Classified research**: Can use AI on sensitive projects
- **Data sovereignty**: Your data never leaves your network
- **Mission assurance**: Works without internet access
- **Compliance**: Meets DoD security requirements

**Competitive Advantage**: Most AI tools are Level 0-1 (cloud-dependent). MARS is Level 3 (classified-capable).

---

## What's Built Today (Nov 2025)




**Foundation** ‚úÖ:
- Docker infrastructure
- Neo4j (knowledge graph)
- Milvus (vector DB) - 80%
- MLflow (experiments)
- LiteLLM (AskSage)
- Ollama (local LLMs)

**Research Tools** ‚úÖ:
- Zotero MCP (100%)
- GitLab MCP (50%, Phase 6A)
- PlantUML/SysML (100%)








**Agents** ‚úÖ:
- DocCzar (doc validation)
- TestCzar (test coordination)
- KG Agent (REQUIREMENT ingestion)

**Dev Infrastructure** ‚úÖ:
- E6: Containerized dev (Docker-in-Docker)
- E8: Parallel orchestration (5-25 sessions)
- E13: Sprint protection (56 tests)
- **434+ tests** across codebase




---

## Roadmap (v1.0: Feb-Mar 2026)

| Component | Status | % | Notes |
|-----------|--------|---|-------|
| **C2** (Zotero) | ‚úÖ COMPLETE | 100% | Production-ready |
| **C6** (SysML) | ‚úÖ COMPLETE | 100% | Diagram generation |
| **C16** (RAG) | ‚úÖ MERGED | 100% | Semantic search |
| **C3** (GitLab) | üöß ACTIVE | 50% | Phase 6A (79 tools) |
| **C4** (Infra) | üöß ACTIVE | 87% | 16/20 enhancements |
| **C11** (LangGraph) | üöß ACTIVE | HITL P4 | Orchestration |
| **C1** (LiteLLM) | ‚è∏Ô∏è BLOCKED | 75% | AskSage streaming |
| **C5** (Lit Research) | ‚è∏Ô∏è PLANNED | Q1 2025 | Orchestrator + monitor |
| **C12-C15** | ‚è∏Ô∏è PLANNED | Pending | Coder, pub-writer agents |

**4 complete, 4 active, 9 planned**

---

## Use Cases MARS Accelerates Today




**1. Literature Management** ‚úÖ:
- Zotero integration
- 10 MCP tools
- Bidirectional sync

**2. Documentation** ‚úÖ:
- DocCzar validates 109 docs
- Broken links, citations
- Standards enforcement








**3. Knowledge Graph** ‚úÖ:
- Neo4j tracks paper ‚Üí requirement ‚Üí design ‚Üí experiment
- REQUIREMENT ingestion
- Cross-domain synthesis

**4. Semantic Search** ‚è≥ 80%:
- ~40% token reduction
- Automatic context retrieval
- (Blocked: upstream MCP bug #226)




---

## What Makes MARS Different?

| Feature | LangGraph/AutoGen | Cloud AI | Custom GPT | **MARS** |
|---------|------------------|----------|------------|---------|
| **Type** | Framework (DIY) | Platform | Single-agent | **Complete system** |
| **Infrastructure** | ‚ùå You provide | ‚òÅÔ∏è Vendor | ‚òÅÔ∏è Cloud | ‚úÖ **Self-hosted** |
| **Orchestration** | ‚úÖ Yes (DIY) | ‚ö†Ô∏è Limited | ‚ùå No | ‚úÖ **LangGraph built-in** |
| **Governance** | ‚ùå You build | ‚ö†Ô∏è Vendor | ‚ùå None | ‚úÖ **Provenance** |
| **Air-Gap** | ‚ö†Ô∏è Possible | ‚ùå No | ‚ùå No | ‚úÖ **100% capable** |
| **Research-Specific** | ‚ùå Generic | ‚ùå Enterprise | ‚ùå Generic | ‚úÖ **Research workflows** |
| **Lock-In** | ‚úÖ No | ‚ùå Yes | ‚ùå Yes | ‚úÖ **Open standards** |

**Unique**: Research-first + Orchestration + Governance + Independence + Classified

---

## Extensibility: 50+ MCP Integrations

**Modularity**: Each integration **~1 hour** (vs. ~80 hours custom)

| Category | Tools | Status |
|----------|-------|--------|
| **Research** | ROS2, SLURM, Overleaf, LabView, MATLAB, SolidWorks | Planned |
| **Data** | PubMed, IEEE, Web of Science, arXiv | Planned |
| **Lab Mgmt** | eLabFTW, Benchling, LabArchives | Planned |
| **Collab** | Slack, Teams, Jira, Confluence | Planned |
| **Hardware** | Oscilloscopes, spectrometers, microscopes | Planned |
| **Simulation** | ANSYS, COMSOL, OpenFOAM, GROMACS | Planned |
| **Current** | Zotero (lit), GitLab (project) | ‚úÖ Operational |

**Timeline**: 3-4 weeks per integration (testing, not coding)

---

## MARS Standards & Protocols




**Communication**:
- **Agent-to-Agent (A2A)**: GraphQL federation (in dev)
- **Agent-to-Tool (MCP)**: Model Context Protocol (operational)
- **Human-to-Agent**: Conversational + approval gates

**Observability**:
- Prometheus metrics
- Health endpoints
- X-Trace-Id propagation
- Append-only provenance








**Development** (mars-dev):
- **37 ADRs**: Documented decisions
- **Pre-commit hooks**: Validation, tests
- **E8 orchestration**: 5-25 parallel CCC sessions
- **Session mgmt**: Export/import, git integration

**Security**:
- Deny-by-default (Squid)
- Rootless containers (Sysbox)
- Bearer token auth (DoD PKI)
- Secret redaction




---

## Organizational Expansion

**Phase 1: Pilot** (3-4 months):
- 1-2 research groups adopt foundation
- Prove value in real programs
- Build expertise
- Cost: 2-3 FTE during setup

**Phase 2: Expansion** (6-9 months):
- 5-7 additional groups (parallel)
- Domain-specific agents (materials, chemistry, biology)
- Shared foundation benefits all
- Cost: <0.2 FTE per group (shared infra team)

**Phase 3: Production** (12+ months):
- Organization-wide capability
- Institutional memory compounds
- 3-5√ó force multiplication achieved
- Cost: Shared maintenance (~1-2 FTE)

**Timeline**: **5-7 weeks per new group** (modularity enables parallelization)


# Part 6: Technical Implementation Details

---

## Technical Deep Dive: Overview

**Purpose**: Explain key MARS implementation choices

**Topics**:
1. Git Worktrees - Parallel development
2. AskSage Integration - DoD AI access via LiteLLM
3. Sysbox Runtime - Secure containerization
4. Git Submodules - Dependency management

**Audience**: Technical stakeholders, infrastructure teams

---

# Worktrees: Parallel Development

---

## The Problem: Linear Development Bottleneck

**Traditional Git Workflow**:
```
main branch (work blocked while testing)
    ‚Üì
Create feature branch
    ‚Üì
Make changes (can't work on other features)
    ‚Üì
Test, iterate
    ‚Üì
Merge back to main
    ‚Üì
Switch to next feature
```

**Limitations**:
- ‚ùå One task at a time per repository
- ‚ùå Frequent context switching
- ‚ùå Long-running tasks block other work
- ‚ùå Testing requires checkout/stash dance

---

## Git Worktrees: The Solution

**What is a worktree?**
> Multiple working directories for the same repository, each with its own branch

**Visual Concept**:
```
Main Repository (/workspace/mars-v2)
    ‚Üì branch: main

Worktree 1 (/workspace/mars-v2/mars-dev/worktrees/feature-a)
    ‚Üì branch: feature/a

Worktree 2 (/workspace/mars-v2/mars-dev/worktrees/feature-b)
    ‚Üì branch: feature/b

Worktree 3 (/workspace/mars-v2/mars-dev/worktrees/testing)
    ‚Üì branch: test/integration
```

**All share**: Same .git history, tags, remotes
**Each has**: Own working directory, own branch, own state

---

## Worktrees in MARS Development

![Git Worktrees Workflow](diagrams/png/git-worktrees-workflow.png){ width=85% }

**How MARS Uses Worktrees**:

**Parallel Sprint Development** (E8 Orchestration):
- **5-25 concurrent CCC sessions**, each in own worktree
- Example: Sprint 7 (Wave 2) = 8 worktrees simultaneously
- Each worktree = isolated environment for one task

**Typical MARS Worktree Structure**:
```
mars-v2/                           # Main repo (read-only during sprints)
‚îú‚îÄ‚îÄ mars-dev/worktrees/
‚îÇ   ‚îú‚îÄ‚îÄ c2-zotero-fixes/          # Component 2 work
‚îÇ   ‚îú‚îÄ‚îÄ c4-infrastructure/         # Component 4 enhancements
‚îÇ   ‚îú‚îÄ‚îÄ s7-wave2-agent-scaffold/   # Sprint 7 task
‚îÇ   ‚îú‚îÄ‚îÄ s7-wave2-testing/          # Parallel testing
‚îÇ   ‚îî‚îÄ‚îÄ mars-writing/              # Documentation sessions
```

---

## Worktrees: Benefits for MARS

**1. True Parallel Development**:
- Work on C2 (Zotero) while C4 (Infrastructure) builds
- Run tests in one worktree, write code in another
- Documentation in one worktree, implementation in another

**2. No Context Loss**:
- Each worktree preserves full IDE state
- Terminal history stays relevant
- No constant branch switching

**3. Isolated Testing**:
- Test changes without affecting main development
- Multiple test configurations simultaneously
- Integration testing in dedicated worktree

**4. Sprint Orchestration** (E8):
- 5-25 parallel CCC sessions (one per worktree)
- Zellij dashboard shows all sessions
- Merge queue for controlled integration

**Productivity Gain**: **40-60% faster** multi-task development

---

## Worktrees: Command Primer

**Create a worktree**:
```bash
# From main repository
git worktree add ../worktrees/feature-x -b feature/x

# Result:
# - New directory: ../worktrees/feature-x
# - New branch: feature/x (checked out in worktree)
# - Ready to work immediately
```

**List all worktrees**:
```bash
git worktree list

# Output:
# /workspace/mars-v2              abc1234 [main]
# /workspace/mars-v2/../feature-x def5678 [feature/x]
```

**Work in worktree** (completely isolated):
```bash
cd ../worktrees/feature-x
vim code.py               # Make changes
git add code.py
git commit -m "feat: Add feature X"
# Main repo unaffected, other worktrees unaffected
```

**Merge back to main**:
```bash
cd /workspace/mars-v2    # Back to main repo
git merge feature/x      # Integrate changes
```

---

## Worktrees: MARS E8 Integration

**E8 Parallel Orchestration** uses worktrees for **sprint management**:

**Sprint Planning** (`mars-dev/sprints/s7-wave2/sprint.yaml`):
```yaml
tasks:
  - id: T1-agent-scaffold
    worktree: s7-wave2-agent-scaffold
    branch: feat/s7-wave2-agent-scaffold

  - id: T2-testing
    worktree: s7-wave2-testing
    branch: feat/s7-wave2-testing
```

**Automated Worktree Creation**:
```bash
# Script: mars-dev/scripts/create-worktrees.sh
./mars-dev/scripts/create-worktrees.sh sprints/s7-wave2/sprint.yaml

# Creates 8 worktrees, 8 branches, 8 Zellij panes
# Each ready for parallel CCC session
```

**Result**: **8 tasks progressing simultaneously** instead of sequentially

---

## Worktrees: Real-World MARS Example

**Scenario**: Sprint 7 Wave 2 (8 parallel tasks)

**Setup** (1 command):
```bash
./mars-dev/scripts/launch-sprint.py sprints/s7-wave2/sprint.yaml
```

**What Happens**:
1. Creates 8 worktrees (one per task)
2. Creates 8 branches (one per task)
3. Launches Zellij dashboard (8 panes)
4. Starts 8 CCC sessions (one per pane)
5. Loads task context into each session

**Parallel Execution**:
- Pane 1: Agent scaffolding
- Pane 2: Integration testing
- Pane 3: Documentation updates
- Pane 4: ADR authoring
- Pane 5-8: Other tasks

**Merge Strategy**: Controlled queue (one worktree at a time)

**Productivity**: 8 tasks in **parallel** vs. **sequential** = **5-7√ó faster sprints**

---

# AskSage Integration

---

## The Challenge: DoD AI Access

**Problem**: Commercial AI services not allowed on classified networks

**Requirements**:
- ‚úÖ Air-gap capable (no internet)
- ‚úÖ DoD accredited (ATO/security review)
- ‚úÖ Data never leaves network
- ‚úÖ Vendor lock-in avoidance

**Traditional Solution**: Build everything from scratch
**MARS Solution**: AskSage + LiteLLM abstraction

---

## What is AskSage?

**AskSage**: Navy-hosted AI service (GPT-4, Claude, Llama)

**Architecture**:
```
Navy CAPRA Endpoint (https://capra.flankspeed.dso.mil)
    ‚Üì (DoD PKI authentication)
AskSage API Gateway
    ‚Üì (routes to available models)
Commercial AI Providers (Azure OpenAI, AWS Bedrock)
    ‚Üì (via FedRAMP connections)
LLM Models (GPT-4, Claude 3.5, Llama 3)
```

**Key Features**:
- üîê **DoD PKI authentication** (CAC/ECA required)
- üõ°Ô∏è **FedRAMP authorized** connections
- üìä **Usage tracking** and audit logs
- üö´ **No training** on your data (contractual)
- üí∞ **Government rates** (volume discounts)

---

## AskSage vs. Commercial Anthropic

| Feature | **AskSage** | **Commercial Anthropic** |
|---------|-------------|-------------------------|
| **Access** | DoD PKI (CAC/ECA) | API key (anyone) |
| **Network** | FedRAMP connection | Public internet |
| **Data Policy** | No training (contract) | No training (policy) |
| **Cost** | Gov't rates (~50% less) | Standard API rates |
| **Models** | GPT-4, Claude, Llama | Claude only |
| **Classified** | ‚úÖ Allowed (with ATO) | ‚ùå Prohibited |
| **Air-Gap** | ‚ö†Ô∏è Partial (local fallback) | ‚ùå Not possible |
| **Audit** | ‚úÖ Full DoD logging | ‚ö†Ô∏è Limited |

**MARS Approach**: Use AskSage for classified, Anthropic for unclassified (LiteLLM handles both)

---

## The LiteLLM Shim: Why It Matters

**Problem**: Each AI provider has different API

**OpenAI API**:
```python
import openai
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**Anthropic API**:
```python
import anthropic
response = anthropic.Anthropic().messages.create(
    model="claude-3-5-sonnet",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**AskSage API**:
```python
import requests
response = requests.post(
    "https://capra.flankspeed.dso.mil/v1/chat/completions",
    headers={"Authorization": f"Bearer {pki_token}"},
    json={"model": "gpt-4", "messages": [...]}
)
```

**All different!** Hard-coded vendor = lock-in

---

## LiteLLM: Unified AI Gateway

![LiteLLM Architecture](diagrams/png/litellm-architecture.png){ width=85% }

**LiteLLM**: Translates all providers ‚Üí OpenAI-compatible API

**MARS Architecture**:
```
MARS Agents
    ‚Üì (OpenAI API format - always)
LiteLLM Proxy (localhost:4000)
    ‚Üì (routes based on config)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
AskSage        Anthropic         Ollama
(via CAPRA)    (commercial)      (local GPU)
```

**Configuration** (`litellm-config.yaml`):
```yaml
model_list:
  - model_name: claude-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_base: https://capra.flankspeed.dso.mil/v1
      api_key: ${ASKSAGE_API_KEY}

  - model_name: claude-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}
```

**MARS Agents**: Just call `localhost:4000/v1/chat/completions` ‚Üí LiteLLM handles routing

---

## LiteLLM Benefits for MARS

**1. Vendor Independence**:
- Switch providers without code changes
- Use AskSage on classified, Anthropic on unclass
- Fallback to local Ollama if network down

**2. Unified Interface**:
- All agents use same API (OpenAI format)
- No vendor-specific code
- Simplified agent development

**3. Cost Optimization**:
- Route expensive calls to cheaper models
- Load balancing across providers
- Usage tracking and quotas

**4. Security & Compliance**:
- Single point of authentication
- Centralized audit logging
- API key rotation without code changes

**5. Air-Gap Readiness**:
- Swap AskSage ‚Üí Ollama (config change only)
- No agent code modifications
- Seamless transition to offline operation

---

## LiteLLM Configuration Example

**MARS uses 3 providers simultaneously**:

```yaml
# litellm-config.yaml
model_list:
  # AskSage (DoD classified work)
  - model_name: gpt-4-asksage
    litellm_params:
      model: azure/gpt-4
      api_base: https://capra.flankspeed.dso.mil/v1
      api_key: os.environ/ASKSAGE_API_KEY

  # Anthropic (unclassified work)
  - model_name: claude-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY

  # Ollama (air-gap fallback)
  - model_name: local-llama
    litellm_params:
      model: ollama/llama3.1:70b
      api_base: http://localhost:11434
```

**Agent Code** (same for all):
```python
import openai
client = openai.OpenAI(base_url="http://localhost:4000/v1")

response = client.chat.completions.create(
    model="claude-sonnet",  # LiteLLM routes appropriately
    messages=[{"role": "user", "content": "Analyze this data"}]
)
```

**No vendor-specific code!**

---

## AskSage Authentication Flow

**Challenge**: AskSage requires DoD PKI (CAC/ECA)

**MARS Solution**:

**1. Manual Token Acquisition** (one-time setup):
```bash
# User authenticates with CAC/ECA via browser
# Receives JWT token (valid 30 days)
export ASKSAGE_API_KEY="eyJ0eXAiOiJKV1QiLCJhbGc..."

# Saved in mars-env.config (sourced at session start)
```

**2. LiteLLM Token Refresh** (automatic):
```yaml
# litellm-config.yaml
general_settings:
  asksage_token_refresh:
    enabled: true
    refresh_endpoint: https://capra.flankspeed.dso.mil/auth/refresh
    refresh_interval: 86400  # 24 hours
```

**3. Fallback to Anthropic** (if token expires):
```python
# LiteLLM automatically tries alternate providers
# Config: fallbacks = ["claude-sonnet", "local-llama"]
```

**Result**: Minimal user intervention, seamless failover

---

# Sysbox: Secure Containers

---

## The Container Security Challenge

**Standard Docker** (privileged containers):
```
Container (runs as root inside)
    ‚Üì (has full system access)
Host Kernel
    ‚Üì (vulnerable to container escape)
Host Filesystem, Network, Devices
```

**Problem**: Root inside container = root on host (with escape exploits)

**Traditional Solutions**:
- **Rootless Docker**: Docker daemon runs as user (complex setup)
- **User Namespaces**: Map root ‚Üí user (limited compatibility)
- **SELinux/AppArmor**: Mandatory access control (complex policies)

**MARS Solution**: **Sysbox** runtime

---

## What is Sysbox?

**Sysbox**: Container runtime that provides **system-level isolation**

**Key Feature**: Containers that **feel like VMs** but are still containers

**Magic Trick**:
- Container **thinks** it has full system access (systemd, Docker-in-Docker)
- Host **knows** container is isolated (UID/GID mapping, namespaces)
- Security **improved** (no privileged mode needed)

**Created by**: Nestybox (acquired by Docker, 2022)
**Used by**: GitLab CI (Docker-in-Docker), Tailscale (system containers)

---

## Docker Runtimes: Standard vs. Sysbox

**Standard Docker (runc runtime)**:
```
docker run -v /var/run/docker.sock:/var/run/docker.sock myimage
```
- ‚ö†Ô∏è Container has **full Docker control** (can break out)
- ‚ö†Ô∏è Mounted `/var/run/docker.sock` = root on host

**Rootless Docker** (rootless daemon):
```
dockerd-rootless.sh
docker run myimage
```
- ‚úÖ Docker daemon runs as user (no root)
- ‚ùå **Complex setup** (slirp4netns, uidmap, XDG_RUNTIME_DIR)
- ‚ùå **Limited features** (no privileged ports, no systemd)

**Sysbox Runtime** (MARS approach):
```
docker run --runtime=sysbox-runc myimage
```
- ‚úÖ **Simple**: Standard Docker workflow
- ‚úÖ **Secure**: UID/GID remapping automatic
- ‚úÖ **Compatible**: Full Docker-in-Docker support
- ‚úÖ **Systemd**: Works inside containers

---

## Sysbox Architecture

![Sysbox Architecture](diagrams/png/sysbox-architecture.png){ width=85% }

**How Sysbox Works**:

**Layer 1: Host Docker Daemon** (runs as root, manages containers)
```
dockerd (system daemon, TCP socket at localhost:9088)
```

**Layer 2: Sysbox Runtime** (intercepts container creation)
```
sysbox-runc (OCI runtime)
    ‚Üì (creates user namespace)
sysbox-fs (FUSE filesystem)
    ‚Üì (emulates /proc, /sys)
sysbox-mgr (resource management)
```

**Layer 3: Container** (isolated environment)
```
Container sees: UID 0 (root), full /proc, /sys, can run Docker
Host sees: UID 100000+ (unprivileged user), namespaced
```

**Key Insight**: Container **believes** it's privileged, but host **enforces** unprivileged operation

---

## DOCKER_HOST: Understanding Docker Sockets

**The Docker Socket Landscape**:

**1. System Docker Socket** (privileged):
```
/var/run/docker.sock (owned by root:docker)
```
- Default Docker daemon socket
- Requires `docker` group membership or root
- **MARS uses this** (via Sysbox isolation)

**2. Rootless Docker Socket** (user):
```
/run/user/1000/docker.sock (owned by user)
```
- Rootless daemon socket
- User-specific (XDG_RUNTIME_DIR)
- **MARS doesn't use** (too complex for multi-user)

**3. Remote Docker Socket** (TCP):
```
tcp://localhost:2375 (insecure)
tcp://localhost:2376 (TLS)
```
- Network-accessible Docker API
- Used for remote management
- **MARS uses TCP in dev** (localhost:9088 for mars-dev)

---

## MARS Docker Architecture

**MARS uses 2 Docker configurations**:

::::: {.columns}
:::: {.column width="48%"}

**Production** (Sysbox):
```
DOCKER_HOST=unix:///var/run/docker.sock
DOCKER_RUNTIME=sysbox-runc
```

**Benefits**:
- System Docker daemon
- Sysbox security isolation
- Standard workflow

**Use Case**:
- Research deployments
- Multi-user environments
- Classified networks

::::
:::: {.column width="48%"}

**Development** (mars-dev):
```
DOCKER_HOST=tcp://localhost:9088
DOCKER_RUNTIME=sysbox-runc
```

**Benefits**:
- Isolated from system Docker
- Won't interfere with host
- Safe for experimentation

**Use Case**:
- Building MARS itself
- Testing infrastructure
- Parallel dev environments

::::
:::::

**Key Point**: Same Sysbox runtime, different daemon endpoints

---

## Docker Image Caching: Sysbox Behavior

**Challenge**: Sysbox uses **per-container** image stores

**Standard Docker** (shared cache):
```
/var/lib/docker/
‚îú‚îÄ‚îÄ image/          # All containers share images
‚îú‚îÄ‚îÄ overlay2/       # Shared layers
‚îî‚îÄ‚îÄ containers/     # Container-specific data
```
**Sysbox** (isolated cache):
```
/var/lib/sysbox/
‚îú‚îÄ‚îÄ container-1/
‚îÇ   ‚îî‚îÄ‚îÄ var/lib/docker/  # Container 1's images
‚îú‚îÄ‚îÄ container-2/
‚îÇ   ‚îî‚îÄ‚îÄ var/lib/docker/  # Container 2's images (duplicate!)
‚îî‚îÄ‚îÄ container-3/
    ‚îî‚îÄ‚îÄ var/lib/docker/  # Container 3's images (duplicate!)
```

**Implication**: Each Sysbox container has **own Docker image cache**

---

## Image Caching: MARS Optimization

**Problem**: 3 containers √ó 2 GB images = 6 GB storage waste

**MARS Solution**: Strategic image caching

**Approach 1: Pre-pull in Host**:
```bash
# On host (before creating Sysbox containers)
docker pull python:3.11-slim
docker pull node:20-alpine
docker pull postgres:15

# Sysbox containers inherit from /var/lib/docker/
# (via sysbox-fs mount magic)
```

**Approach 2: Shared Volume** (experimental):
```yaml
# docker-compose.yml
services:
  mars-dev:
    runtime: sysbox-runc
    volumes:
      - docker-cache:/var/lib/docker:ro  # Read-only shared cache
```

**Approach 3: Build Once, Run Many**:
```bash
# Build MARS images on host
docker build -t mars/litellm:latest -f litellm/Dockerfile .

# Sysbox containers can docker pull from host registry
# (if registry is accessible)
```

**Current MARS Status**: Uses Approach 1 (pre-pull) + periodic cleanup

---

## Sysbox vs. Rootless Docker: Comparison

| Feature | **Sysbox** (MARS) | **Rootless Docker** |
|---------|-------------------|---------------------|
| **Setup Complexity** | ‚úÖ Simple (install runtime) | ‚ùå Complex (daemon setup) |
| **Docker-in-Docker** | ‚úÖ Full support | ‚ö†Ô∏è Limited |
| **Systemd Support** | ‚úÖ Yes | ‚ùå No |
| **Privileged Ports** | ‚úÖ Yes (remapped) | ‚ùå No (>1024 only) |
| **UID Mapping** | ‚úÖ Automatic | ‚ö†Ô∏è Manual config |
| **Image Caching** | ‚ö†Ô∏è Per-container | ‚úÖ Shared |
| **Security** | ‚úÖ Strong isolation | ‚úÖ Strong isolation |
| **Multi-User** | ‚úÖ Works well | ‚ö†Ô∏è Per-user daemon |

**Why MARS chose Sysbox**: Simpler setup, full Docker features, better multi-user

---

## Sysbox in MARS: Practical Example

**Scenario**: Running MARS development environment (E6)

**Without Sysbox** (privileged container):
```bash
docker run --privileged \
  -v /var/run/docker.sock:/var/run/docker.sock \
  mars/dev:latest

# ‚ö†Ô∏è Container can:
# - Access host Docker daemon (full control)
# - Escape to host via vulnerabilities
# - Read/write host filesystem
```

**With Sysbox** (MARS approach):
```bash
docker run --runtime=sysbox-runc \
  mars/dev:latest

# ‚úÖ Container can:
# - Run Docker inside (isolated daemon)
# - Use systemd (process manager)
# - Feel like full system
#
# ‚ùå Container cannot:
# - Access host Docker daemon
# - Break out to host
# - See other containers' data
```

**Security Gain**: **Defense in depth** without functionality loss

---

# Git Submodules

---

## The Dependency Management Challenge

**MARS has 2 types of dependencies**:

**1. Research Project Dependencies** (outward):
- How do research projects **use** MARS?
- Where does MARS code live in research repos?
- How to update MARS version?

**2. MARS Internal Dependencies** (inward):
- How does MARS **use** external tools (LiteLLM, GitLab MCP, Zotero)?
- How to version-control external dependencies?
- How to ensure reproducible builds?

**Solution**: Git submodules (both directions)

---

## What are Git Submodules?

**Git Submodule**: A git repository embedded inside another git repository

**Visual Concept**:
```
Main Repository (research-project/)
‚îú‚îÄ‚îÄ .git/                 # Main repo's git metadata
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/       # Your research code
‚îÇ   ‚îî‚îÄ‚îÄ framework/        # MARS (git submodule)
‚îÇ       ‚îú‚îÄ‚îÄ .git/         # MARS's git metadata (pointer)
‚îÇ       ‚îî‚îÄ‚îÄ core/         # MARS source code
‚îî‚îÄ‚îÄ .gitmodules           # Submodule configuration
```

**Key Properties**:
- **Specific commit**: Submodule tracks exact MARS version (e.g., commit abc123)
- **Independent history**: MARS and research-project have separate git histories
- **Reproducible**: `git clone --recursive` gets exact versions

---

## Git Submodules: How Research Projects Use MARS

![Git Submodules - Research Project](diagrams/png/git-submodules-research.png){ width=85% }

**Research Project Structure**:
```
battery-research/                    # Your research repository
‚îú‚îÄ‚îÄ .git/
‚îú‚îÄ‚îÄ .gitmodules                      # Defines MARS submodule
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ framework/                   # MARS (git submodule)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bin/mars                 # MARS CLI
‚îÇ   ‚îî‚îÄ‚îÄ battery_sim/                 # Your research code
‚îú‚îÄ‚îÄ data/                            # Your research data
‚îú‚îÄ‚îÄ results/                         # Your experiment results
‚îî‚îÄ‚îÄ README.md
```

**.gitmodules file**:
```ini
[submodule "src/framework"]
    path = src/framework
    url = https://github.com/nasa/mars-v2.git
    branch = v1.0
```

**Setup** (one-time):
```bash
git submodule add https://github.com/nasa/mars-v2.git src/framework
git commit -m "Add MARS framework as submodule"
```

---

## Submodules: Research Project Workflow

**Initial Clone** (new team member):
```bash
# Clone research project
git clone https://gitlab.example.com/research/battery-sim.git
cd battery-sim

# Initialize submodules (gets MARS)
git submodule update --init --recursive

# Result: src/framework/ now contains MARS
```

**Update MARS Version**:
```bash
# Navigate to MARS submodule
cd src/framework

# Pull latest MARS changes
git pull origin main

# Return to research project
cd ../..

# Commit updated MARS version
git add src/framework
git commit -m "Update MARS to v1.1"
```

**Benefit**: Research project tracks **exact MARS version** (reproducible results)

---

## Git Submodules: How MARS Uses External Dependencies

![Git Submodules - MARS Dependencies](diagrams/png/git-submodules-mars.png){ width=85% }

**MARS Repository Structure**:
```
mars-v2/                             # MARS repository
‚îú‚îÄ‚îÄ .git/
‚îú‚îÄ‚îÄ .gitmodules                      # Defines 5 external dependencies
‚îú‚îÄ‚îÄ core/
‚îú‚îÄ‚îÄ modules/
‚îú‚îÄ‚îÄ external/                        # External dependencies (submodules)
‚îÇ   ‚îú‚îÄ‚îÄ litellm/                     # Git submodule ‚Üí LiteLLM fork
‚îÇ   ‚îú‚îÄ‚îÄ gitlab-mcp/                  # Git submodule ‚Üí GitLab MCP fork
‚îÇ   ‚îú‚îÄ‚îÄ zotero-selfhost/             # Git submodule ‚Üí Zotero Docker
‚îÇ   ‚îú‚îÄ‚îÄ claude-extractor/            # Git submodule ‚Üí Session exporter
‚îÇ   ‚îî‚îÄ‚îÄ generic-infra/               # Git submodule ‚Üí Infrastructure patterns
‚îî‚îÄ‚îÄ bin/
```

**Why Submodules for External Dependencies?**
- ‚úÖ **Version locking**: MARS tracks exact commit of each dependency
- ‚úÖ **Reproducible builds**: `git clone --recursive` gets everything
- ‚úÖ **Offline development**: All dependencies in one clone
- ‚úÖ **Fork management**: MARS uses forks with custom patches

---

## MARS External Dependencies

**5 External Repositories** (all managed as submodules):

| Dependency | Type | Purpose | Fork? |
|------------|------|---------|-------|
| **LiteLLM** | Python/Docker | AI gateway (AskSage/CAPRA) | ‚úÖ Yes (AskSage provider) |
| **GitLab MCP** | Node.js MCP | 79 GitLab tools | ‚úÖ Yes (fixes/features) |
| **Zotero Server** | Docker Compose | Self-hosted Zotero | ‚ùå No (upstream) |
| **Zotero MCP** | Python MCP | Literature mgmt tools | ‚úÖ Yes (10 tools) |
| **Claude Extractor** | Python CLI | CCC session export | ‚ùå No (upstream) |
| **Generic Infra** | Patterns | Reusable infra templates | ‚úÖ Yes (extracted from MARS) |

**Setup**:
```bash
# One command gets all dependencies
git clone --recursive https://github.com/nasa/mars-v2.git
```

---

## Submodules: MARS Update Workflow

**Update External Dependency**:

**Scenario**: LiteLLM releases new version with bug fixes

**Workflow**:
```bash
# Navigate to submodule
cd external/litellm

# Check current commit
git log --oneline -1
# Output: abc123 Add AskSage provider support

# Pull upstream changes
git fetch origin
git merge origin/main

# Test changes
cd ../..
pytest tests/test_litellm_integration.py

# Commit updated submodule
git add external/litellm
git commit -m "Update LiteLLM to v1.2.3 (bug fixes)"
```

**Benefit**: Controlled dependency updates, full testing before integration

---

## Submodules vs. Package Managers

**Why not use pip/npm/docker pull?**

| Approach | **Git Submodules** | **Package Managers** |
|----------|-------------------|---------------------|
| **Offline** | ‚úÖ Works (bundled) | ‚ùå Needs internet |
| **Reproducibility** | ‚úÖ Exact commit | ‚ö†Ô∏è Version ranges |
| **Custom Patches** | ‚úÖ Easy (fork) | ‚ùå Hard (patch files) |
| **Air-Gap** | ‚úÖ Full support | ‚ùå Requires registry |
| **Version Lock** | ‚úÖ Explicit commit | ‚ö†Ô∏è Lock files (can drift) |
| **Security Review** | ‚úÖ Full source visible | ‚ö†Ô∏è Binary/compiled |

**MARS Choice**: Submodules for **critical dependencies**, package managers for **libraries**

**Example**:
- `external/litellm/` ‚Üí **Submodule** (custom AskSage provider)
- `pip install requests` ‚Üí **Package manager** (standard library)

---

## Submodules: Common Workflows

**1. Initial Setup** (new MARS deployment):
```bash
git clone --recursive https://github.com/nasa/mars-v2.git
cd mars-v2
./mars-dev/scripts/setup-external-deps.sh
```

**2. Update All Submodules**:
```bash
git submodule update --remote --merge
```

**3. Check Submodule Status**:
```bash
git submodule status
# Output shows commit hash and branch for each submodule
```

**4. Work on Submodule** (develop patch):
```bash
cd external/litellm
git checkout -b fix/asksage-streaming
# Make changes, test, commit
git push origin fix/asksage-streaming
# Return to MARS, update submodule reference
cd ../..
git add external/litellm
git commit -m "Update LiteLLM with AskSage streaming fix"
```

---

## Submodules: Best Practices (MARS Approach)

**1. Pin to Specific Commits** (not branches):
```bash
# Good: Reproducible
cd external/litellm
git checkout abc123  # Specific commit
cd ../..
git add external/litellm

# Avoid: Can change unexpectedly
git submodule update --remote  # Pulls latest, may break
```

**2. Test Before Updating**:
```bash
# Update submodule
cd external/litellm && git pull

# Run tests
cd ../.. && pytest tests/test_litellm_*.py

# Only commit if tests pass
git add external/litellm && git commit -m "..."
```

**3. Document Dependencies**:
```yaml
# external-dependencies.yaml
dependencies:
  litellm:
    type: git-submodule
    path: external/litellm
    commit: abc123
    purpose: AskSage/CAPRA AI gateway
    status: Production
```

---

## Submodules: MARS Dependency Graph

```
Research Project (battery-sim)
    ‚Üì (git submodule)
MARS v2 (mars-v2)
    ‚Üì (5 git submodules)
    ‚îú‚îÄ‚îÄ LiteLLM (AI gateway)
    ‚îú‚îÄ‚îÄ GitLab MCP (project mgmt)
    ‚îú‚îÄ‚îÄ Zotero MCP (literature)
    ‚îú‚îÄ‚îÄ Zotero Server (self-hosted)
    ‚îî‚îÄ‚îÄ Generic Infra (patterns)
```

**Reproducibility Chain**:
1. Research project pins MARS commit (e.g., v1.0.2)
2. MARS pins dependency commits (LiteLLM abc123, GitLab xyz789)
3. `git clone --recursive` = **exact versions, always**

**Air-Gap Capability**:
- Single clone = all dependencies (no network needed)
- Tarball distribution = fully offline installation
- Security review = all source code visible

---

## Submodules: Troubleshooting

**Problem 1**: Submodule not initialized
```bash
# Symptom: external/litellm/ is empty
# Fix:
git submodule update --init --recursive
```

**Problem 2**: Submodule detached HEAD
```bash
# Symptom: (HEAD detached at abc123)
# Fix (if you want to track a branch):
cd external/litellm
git checkout main
git pull
cd ../..
git add external/litellm
```

**Problem 3**: Merge conflicts in submodule
```bash
# Symptom: Conflict in submodule pointer
# Fix:
cd external/litellm
git status  # See what's conflicting
# Resolve, then:
cd ../..
git add external/litellm
```

---

# Summary: Technical Implementation

---

## Technical Choices: Recap

**1. Git Worktrees**: **40-60% faster** parallel development
- 5-25 concurrent CCC sessions
- No context switching
- E8 sprint orchestration

**2. AskSage + LiteLLM**: **Vendor independence**
- DoD classified AI access
- Fallback to Anthropic/Ollama
- Zero code changes to swap providers

**3. Sysbox Runtime**: **Secure containerization**
- Docker-in-Docker without privileged mode
- UID/GID isolation automatic
- Full systemd support

**4. Git Submodules**: **Reproducible builds**
- Exact dependency versions
- Air-gap capable
- Offline development

**Result**: Research-grade platform with DoD-level security

---

## MARS Architecture: Three-Layer System

**Separation of Concerns**

::::: {.columns}
:::: {.column width="30%"}

### mars-rt
**Runtime CLI**

**Purpose**: Run research projects

**Users**: Research teams

**Commands**:
- `mars up` - Deploy services
- `mars agents list`
- `mars services info`

**Location**: Production/research environments

::::
:::: {.column width="30%"}

### mars-dev
**Development CLI**

**Purpose**: Build MARS itself

**Users**: MARS developers

**Commands**:
- `mars-dev up` - Start E6 container
- `mars-dev build`
- `mars-dev doctor`

**Location**: E6 dev container

::::
:::: {.column width="30%"}

### mars-user-plugin
**Personal Customization**

**Purpose**: Customize dev environment

**Users**: Individual developers

**Capabilities**:
- Dotfiles (.vimrc, .bashrc)
- Custom scripts
- IDE configs
- Lifecycle hooks

**Location**: `external/mars-user-plugin/`

::::
:::::

---

## mars-user-plugin: Developer Experience Layer

**Problem**: Every developer has different preferences

**Solution**: Plugin system for personal customization

**Key Features**:

1. **Auto-Mount System**: Drop files ‚Üí auto-mounted in E6 container
   ```
   external/mars-user-plugin/
   ‚îî‚îÄ‚îÄ mounted-files/
       ‚îî‚îÄ‚îÄ root/
           ‚îú‚îÄ‚îÄ .vimrc    (660 ‚Üí rw mount)
           ‚îî‚îÄ‚îÄ .bashrc   (640 ‚Üí ro mount)
   ```

2. **Lifecycle Hooks**: Hook into mars-dev events
   - `pre-up.sh` - Before container starts
   - `container-startup.sh` - During startup
   - `post-build.sh` - After build completes

3. **Git-Tracked**: Personal configs tracked in separate repo
   - Share across MARS deployments
   - Version control your dev environment
   - Zero impact on mars-v2 repository

---

## Three-Layer Relationship

![Three-Layer Architecture](diagrams/png/three-layer-architecture.png)

**Key Principle**: Clean separation
- **mars-rt**: What research teams use
- **mars-dev**: How we build MARS
- **mars-user-plugin**: How each developer personalizes their workflow

---

## mars-user-plugin: Quick Start

**5-Minute Setup**

```bash
# 1. Clone plugin template
cp -r external/mars-user-plugin-template ~/dev/mars-user
cd ~/dev/mars-user

# 2. Add your dotfiles
cp ~/.vimrc mounted-files/root/.vimrc
chmod 660 mounted-files/root/.vimrc  # rw mount

# 3. Register with mars-dev
cd ~/dev/mars-v2
mars-dev register-plugin ~/dev/mars-user

# 4. Rebuild E6 container
mars-dev down
mars-dev build --no-cache
mars-dev up -d

# 5. Your dotfiles are now in E6!
mars-dev exec bash
vim  # Uses your .vimrc configuration
```

**Benefits**:
- ‚úÖ Personal preferences without polluting mars-v2
- ‚úÖ Share configs across MARS deployments
- ‚úÖ Version control your dev environment
- ‚úÖ Zero learning curve (just drop files)

---

## MARS by the Numbers

**Repository Statistics** (as of 2025-11-05)

::::: {.columns}
:::: {.column width="48%"}

**Code Metrics**:

| Metric | Count |
|--------|-------|
| **Total Modules** | 32 |
| **Python Files** | 2,307 |
| **Lines of Code** | 363,530 |
| **Test Files** | 1,591 |
| **Lines of Tests** | 375,667 |
| **Shell Scripts** | 841 |
| **Config Files** | 1,730 |
| **ADRs** | 37 |

::::
:::: {.column width="48%"}

**Infrastructure**:

| Component | Value |
|-----------|-------|
| **Runtime Containers** | 35 |
| **Exposed Ports** | 8 |
| **Docker Volumes** | 29 |
| **Docker Networks** | 3 |
| **Python Dependencies** | 89 |

**Documentation**:
- **8,144,870 lines** across 8,995 files
- **37 ADRs** documenting key decisions

::::
:::::

**Category Breakdown**:

| Category | Modules | Python Files | LOC | Tests (LOC) | Docs (lines) | ADRs |
|----------|---------|--------------|-----|-------------|--------------|------|
| **Core** | 1 | 78 | 12,689 | 13,817 | 10,043 | 19 |
| **mars-dev** | 1 | 2,115 | 338,481 | 345,620 | 8,116,090 | 10 |
| **Agents** | 8 | 41 | 5,101 | 5,617 | 3,312 | 4 |
| **Services** | 22 | 73 | 7,259 | 10,613 | 15,425 | 4 |
| **TOTAL** | **32** | **2,307** | **363,530** | **375,667** | **8,144,870** | **37** |

---


# Appendices

---

## Appendix A: Glossary (Plain Language)

| Term | Definition |
|------|-----------|
| **LLM** | Large Language Model - pattern engine trained on text |
| **AI Agent** | LLM + tool use + multi-step planning (executes) |
| **MCP** | Model Context Protocol - USB for AI agents |
| **Orchestration** | Automated coordination of specialized AI agents |
| **LangGraph** | Framework for agent orchestration |
| **RAG** | Retrieval-Augmented Generation - semantic search (~40% tokens) |
| **Knowledge Graph** | Relationship database (Neo4j) - paper ‚Üí requirement ‚Üí experiment |
| **Self-Hosted** | Runs on our infrastructure, not cloud |
| **Air-Gap** | Fully offline operation (no internet) |
| **Rootless** | Containers run as non-root user (security) |
| **MCP Server** | Tool that provides capabilities to AI agents via MCP |

---

## Appendix B: Key References (2024)




**Level 1 (Chat AI)**:

1. **GitHub Copilot RCT**
   Microsoft/MIT/Princeton/Wharton
   26% avg productivity, 4,000+ devs
   *Communications of ACM*

2. **Google Enterprise AI**
   Google, 2024
   21% faster task completion
   Large-scale RCT








**Level 2 (AI Agents)**:

3. **AI and Coding Productivity**
   *Science Magazine*, 2024
   40% faster, 18% higher quality
   Peer-reviewed, top-tier

4. **GitHub HTTP Server**
   GitHub/OpenAI, 2023
   55.8% speed improvement
   95 professional developers




---

## Appendix B: References (continued)




**Level 3/4 (Orchestration)**:

5. **McKinsey GenAI Report**
   McKinsey Global Institute, 2024
   30-40% efficiency gains
   Enterprise case studies

6. **BCG Multi-Agent Study**
   Boston Consulting Group, 2024
   45% margin improvement
   Campaign delivery optimization








**Supporting Evidence**:

7. **Stanford HAI Study**
   Stanford Human-Centered AI, 2024
   AI-augmented: 2.3√ó publication rate
   Literature analysis 2020-2024

8. **Anthropic Claude Code**
   Anthropic, 2024
   49% resolution on SWE-bench
   Complex problem solving




**Insight**: Peer-reviewed, large-scale, reproducible evidence of **transformational** gains

---

## Appendix C: MARS Architecture Deep Dive




**Core Services** (Self-Hosted):
- `graph-db` (Neo4j) - Knowledge graph
- `vector-db` (Milvus) - Semantic search
- `object-store` (MinIO) - S3-compatible
- `experiment-tracker` (MLflow) - Logging
- `metrics-store` (Prometheus) - Time-series
- `network-proxy` (Squid) - Security




**AI Integration**:
- `litellm` - Unified API (AskSage, Claude, GPT, local)
- `selfhosted-models` (Ollama) - GPU local LLMs

**Research Tools**:
- `biblio-store` (Zotero) - Literature
- `gitlab-sync` - Project mgmt, 79 tools
- `uml-service` - PlantUML/SysML diagrams




![MARS Architecture](diagrams/png/appendix-c-architecture.png){ width=75% }

---

## Summary: The Path Forward




**Where We Are**:
- Corvette ‚Üí Formula 1
- Ad-hoc AI chat
- No strategy

**Where We Need to Be**:
- Starship Enterprise
- Orchestrated AI teams
- **3-5√ó force multiplication**

**The Window**:
- 12-18 months before irreversible
- **We're at Month 6-8**








**Evidence**:
- Peer-reviewed studies
- **2-5√ó productivity gains**
- Transformational, not incremental

**The Ask**:
1. **Primary**: Commit to organizational investment
2. **Secondary**: Consider MARS as platform

**MARS Status**:
- Foundation operational
- Ready for pilot




**Next Steps**: Leadership decision ‚Üí Pilot ‚Üí Expansion

---

## Questions & Discussion

**Open Topics**:
- Pilot program scope and timeline
- Resource allocation (people, infrastructure, funding)
- Security and compliance review
- Integration with existing workflows
- Domain-specific requirements

**Contact**: Joe Hays, NRL Code 8234

**Thank you for your time and consideration.**

---
